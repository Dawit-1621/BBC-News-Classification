{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "BBC_News_Classification.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sharp-diabetes"
      },
      "source": [
        "# BBC News Classification Project"
      ],
      "id": "sharp-diabetes"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fantastic-boards"
      },
      "source": [
        "There exists a large amount of information being stored in the electronic format. With such data it has\n",
        "become a necessity of such means that could interpret and analyse such data and extract such facts that could\n",
        "help in decision-making.<br>\n",
        "News is easily accessible via content providers such as online news services. A huge amount of information exists in form of text in various diverse areas whose analysis can be beneficial in several areas. Classification is quite a challenging field in text mining as it requires prepossessing steps to convert unstructured data to structured information. With the increase in the number of news it has got difficult for users to access news of his interest which makes it a necessity to categories news so that it could be easily accessed. Categorization refers to grouping that allows easier navigation among articles. Internet news needs to be divided into categories"
      ],
      "id": "fantastic-boards"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "useful-planet"
      },
      "source": [
        "### The Dataset\n",
        "Text documents are one of the richest sources of data for businesses.\n",
        "\n",
        "We’ll use a public dataset from the BBC comprised of 2225 articles, each labeled under one of 5 categories: business, entertainment, politics, sport or tech.\n",
        "\n",
        "The dataset is broken into 1490 records for training and 735 for testing. The goal will be to build a system that can accurately classify previously unseen news articles into the right category.\n"
      ],
      "id": "useful-planet"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overhead-commission"
      },
      "source": [
        "1. **Summary of the project**: We are classifying BBC news articles into five categories using natural Language Processing and Machine Learning. The five news topics are: Politics, Entertainment, Sports, Technology, and Business. The goal of this project is to create a text classifier that will streamline the process of categorizing news publications. \n",
        "<br><br>\n",
        "2. **Summary of the data**: The dataset consists of 2225 news articles extracted from the BBC website between 2004 and 2005. It was published open source by Insight Resources and was collected by UC Davis for research. The class distribution is as follows:\n",
        "```\n",
        "business: 510\n",
        "entertainment: 386\n",
        "politics: 417 \n",
        "sports: 511\n",
        "technology: 401 \n",
        "```"
      ],
      "id": "overhead-commission"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiPvtRhSbA7k",
        "outputId": "dc035f64-b728-4dcc-8350-e86e5bed734d"
      },
      "source": [
        "!pip install pyspark"
      ],
      "id": "FiPvtRhSbA7k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/db/e18cfd78e408de957821ec5ca56de1250645b05f8523d169803d8df35a64/pyspark-3.1.2.tar.gz (212.4MB)\n",
            "\u001b[K     |████████████████████████████████| 212.4MB 77kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 18.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.1.2-py2.py3-none-any.whl size=212880768 sha256=2c6b2e8def6e85cefc86f5fdda9cf2ec86ddb272fd614868abc7dc7c8293ae7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/1b/2c/30f43be2627857ab80062bef1527c0128f7b4070b6b2d02139\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QE9YbbCc0_N",
        "outputId": "fe557924-6c86-4099-aa77-f5d43de1499e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "id": "1QE9YbbCc0_N",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owned-surrey"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import csv\n",
        "import glob\n",
        "import os.path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pyspark import SparkFiles\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import *\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.mllib.classification import *\n",
        "\n",
        "sc = SparkContext.getOrCreate()\n",
        "sqlCtx = SQLContext(sc)"
      ],
      "id": "owned-surrey",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLBn_DtHa0Ms"
      },
      "source": [
        "#Load the dataset\n",
        "def read_file(main_df, category):\n",
        "  for filename in glob.glob('/content/drive/My Drive/AI_Project/NLP/BBC-News-Classification/Data/' + category + '/*'):\n",
        "    df = pd.read_csv(filename, sep = \"\\n\", header = None, quoting = csv.QUOTE_NONE)\n",
        "    df = df.transpose()\n",
        "    df['text'] = df.apply(lambda x: '\\n'.join(x.dropna().astype(str)),axis = 1)\n",
        "    df = df.drop(df.columns[:-1], axis = 1)\n",
        "    df['label'] = category\n",
        "    main_df = pd.concat([main_df, df], ignore_index= True)\n",
        "  return main_df"
      ],
      "id": "YLBn_DtHa0Ms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "VKvf7mVtdmYC",
        "outputId": "d86534cf-775d-4c7d-ef5c-f5287f32954e"
      },
      "source": [
        "#Add respective label\n",
        "df_news = pd.DataFrame(columns = ['text', 'label'])\n",
        "list = ['business', 'politics', 'entertainment', 'sport', 'tech']\n",
        "for genre in list:\n",
        "  df_news = read_file(df_news, genre)\n",
        "df_news.head()"
      ],
      "id": "VKvf7mVtdmYC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, label]\n",
              "Index: []"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUTQpwyIb_0l"
      },
      "source": [
        ""
      ],
      "id": "BUTQpwyIb_0l"
    }
  ]
}